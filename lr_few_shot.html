<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&display=swap" rel="stylesheet">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Patua+One&display=swap" rel="stylesheet">

  <title>Low-Resources Few-Shot learning with VLMs</title>
  <link rel="icon" type="image/png" sizes="32x32" href="images/favicons/favicon-32x32.png">
  <link rel="stylesheet" href="style.css">
</head>

<body>

  <!-- NAVBAR DESKTOP -->

  <div class="navbar-desktop">
    <a class="a-light" style="margin-right: 5%;" href="index.html">HOME</a>

    <a class="a-light" style="margin-right: 5%;" href="projects.html">PROJECTS</a>

    <a class="a-light" style="margin-right: 5%;" href="bio.html">ABOUT ME</a>

    <a class="a-light" style="margin-right: 5%;" href="contacts.html">CONTACT</a>

  </div>

  <!-- NAVBAR MOBILE -->

  <div class="navbar-mobile">
    <a class="a-light" style="margin-bottom: 8px;" href="index.html">HOME</a>

    <a class="a-light" style="margin-bottom: 8px;" href="projects.html">PROJECTS</a>

    <a class="a-light" style="margin-bottom: 8px;" href="bio.html">ABOUT ME</a>

    <a class="a-light" style="margin-bottom: 8px;" href="contacts.html">CONTACT</a>

  </div>


  <div class="empty-space"></div>

  <div class="section-03">
    <div style="max-width: 600px;">
      <h2 style="margin-bottom: 16px;">Low-Resources Few-Shot learning with VLMs</h2>
      <h3 style="margin-bottom: 16px;">Adapting VLMs to Low-Resources contexts</h3>
    </div>

    <div class="empty-space"></div>

    <div class="project-overview">
      <h4>Project Overview</h4>
      <div style="height: 16px; border-bottom: 1px solid rgba(0, 0, 0, 0.1);"></div>
      <div style="height:16px;"></div>
      
      <div class="project-overview-content">
        <div class="project-topic">
          <h3>Problems</h3>
          <p>
            This project addresses the challenges posed by long-tailed data distributions and low-resource scenarios.
            <br><br>
            These situations often result in difficulties for models to generalize effectively across classes with sparse data representation, highlighting the need for robust solutions.
            <br><br>
            Additionally, the project investigates biases and failure cases in Vision-Language Models like CLIP, which often struggle with specific dataset characteristics.
          </p>
        </div>
    
        <div class="project-topic">
          <h3>Methods</h3>
          <p>
            <b>Few Shot Learning;</b><br><br>
            <b>LoRA;</b><br><br>
            <b>BitFit;</b><br><br>
            <b>Meta-Adapter;</b><br><br>
            <b>Data Augmentation;</b><br><br>
          </p>
        </div>
    
        <div class="project-topic">
          <h3>Tools</h3>
          <p>
            <b>Python;</b><br><br>
            <b>PyTorch;</b><br><br>
            <b>Stable Diffusion</b>;<br><br>
          </p>
        </div>
    
        <div class="project-topic">
          <h3>Goals</h3>
          <p>
            The primary objective of this project was to evaluate the effectiveness of few-shot learning methods in addressing long-tailed datasets.
            <br><br>
            Through analyzing failure cases and exploring method combinations, the project aimed to enhance model robustness in low-resource settings and identify opportunities for improving Vision-Language Models like CLIP.
          </p>
        </div>
      </div>
      <div style="height: 16px; border-bottom: 1px solid rgba(0, 0, 0, 0.1);"></div>
      <div style="height:16px;"></div>
      <h5>Developed in collaboration with Alessandro Lorenzi & Omar Facchini (2024).</h5>
    </div>

    <div class="empty-space"></div>

    <div style="max-width: 750px;">
      <div class="empty-space"></div>
      <h4 style="margin-bottom: 16px;">Context</h4>
      <p>
        This project was developed as part of the <b>"Trends and Applications of Computer Vision"</b> course at the University of Trento during the academic year 2023/2024, in the last year of my Master's degree in Artificial Intelligence Systems.
        <br><br>
        The course, taught by professors <b><a href="https://scholar.google.com/citations?user=bqTPA8kAAAAJ&hl=en" class="a-light">Massimiliano Mancini</a></b> and <b><a href="https://scholar.google.com/citations?user=Jg8F7kMAAAAJ&hl=en" class="a-light">Giulia Boato</a></b>, focuses on exploring cutting-edge research topics in computer vision, providing students with the opportunity to engage with real-world challenges and extend state-of-the-art methodologies.
      </p>
    </div>
    
  </div>

  <div class="empty-space-big"></div>

  <!-- PROCESS -->

  <div style="background-color: #dbf1ff">
    <div class="empty-space-big"></div>
    <div class="section-03">
      <h3>Design process</h3>

      <div class="empty-space"></div>

      <div class="project-overview-content">
        <div class="circle">
          <div class="circle-number"></div>
          <h3>1</h3>
          <h4>Literature Review on long-tailed data distribution problem</h4>
        </div>
        <div class="circle">
          <div class="circle-number"></div>
          <h3>2</h3>
          <h4>Implementation of chosen methodologies</h4>
        </div>
        <div class="circle">
          <div class="circle-number"></div>
          <h3>3</h3>
          <h4>Models training</h4>
        </div>
        <div class="circle">
          <div class="circle-number"></div>
          <h3>4</h3>
          <h4>Failure Case Analysis & Intuitions</h4>
        </div>
      </div>
    </div>
    <div class="empty-space-big"></div>
  </div>

  <div class="empty-space-big"></div>

  <div class="section-03">
    <div style="max-width: 750px;">
      <!-- The Problem  -->
      <h4 style="margin-bottom: 16px;"> The Problem</h4>
      <p>
        Real-world datasets often exhibit a long-tailed distribution where a few classes (head classes) are well-represented, while the majority (tail classes) lack sufficient examples. This imbalance creates challenges for machine learning models, which struggle to generalize effectively across all classes. The issue becomes more pronounced in low-resource scenarios, where collecting additional data is impractical. Vision-Language Models like CLIP, while powerful, face difficulties in these settings due to biases and their reliance on extensive pre-trained datasets.
      </p>
      <div class="empty-space"></div>

      <h4 style="margin-bottom: 16px;"> Methodology</h4>   
      <p>
        This project began with a <a href="resources/report_intermediate_trends.pdf" class="a-light">comprehensive literature review</a>, categorizing existing techniques to address long-tailed data distributions into four main approaches:
        <br><br>
        <img src="images/lr_fsl/categories.png" alt="categorization" style="margin-left:25%;  width:50%;">
        <br><br>
        From those categories, 4 relevant techniques were implemented and selected to perform our study:
          <li><a class="a-light" href="https://arxiv.org/abs/2106.09685">Low-Rank Adaptation (LoRA)</a></li>
          <li><a class="a-light" href="https://arxiv.org/abs/2106.10199">Bias-terms Fine-tuning (BitFit)</a></li>
          <li><a class="a-light" href="https://arxiv.org/abs/2311.03774">Meta-Adapter</a></li>
          <li><a class="a-light" href="https://arxiv.org/abs/2401.04716">Label Preserving and Breaking Data Augmentation</a></li>
        </ul>
        <br>
        Each method was implemented and evaluated individually and in combination, focusing on their ability to improve classification performance on long-tailed datasets. Challenging datasets like <b>EuroSAT</b> and <b>Circuit-Diagrams</b> were used to test the methods under real-world conditions.
      </p>
      <div class="empty-space"></div>
      
      <!-- Experiments  -->
      <h4 style="margin-bottom: 16px;"> Experiments</h4>
      <p>
        The experiments were designed to assess model performance, particularly in failure cases. Key steps included:
        <ul>
          <li>Applying methods to a few-shot learning setup using CLIP.</li>
          <li>Evaluating results using metrics such as accuracy, cluster separability (Silhouette Score), and confusion matrices.</li>
          <li>Visualizing model behavior extracting attention maps and inspecting UMAP projections.</li>
        </ul>
        <br>
        ex:
        <img src="images/lr_fsl/zero_vs_lora_bitfit_eurosat.png" alt="example of analysis" style="width:100%; max-height:300px;">
      </p>
      <div class="empty-space"></div>

      <!-- Analysis: EuroSAT  -->
      <h4 style="margin-bottom: 16px;"> Analysis: EuroSAT</h4>
      <p>
        The EuroSAT dataset, which consists of satellite imagery for land use and cover classification, was chosen for its long-tailed yet structured nature. The baseline zero-shot CLIP configuration demonstrated noticeable strengths for certain classes, such as Annual Crop Land and Highway or Road, due to distinct visual patterns.
        <br><br>
        More interesting is the visualization of failure cases, which we categorized into two main patterns:
        <ol>
            <li><strong>Presence of significant patterns:</strong> For instance, straight lines make the model tilt towards predicting the class in which such geometric features are present. In the top-left image, we have a sample of Annual Crop Land presenting a "striped" pattern, which leads to misclassification with Highway or Road. The same happens for image 3.</li>
            <li><strong>Struggle in identifying the image's subject:</strong> There are samples (top-right, for instance) in which there are sub-regions presenting other classes, which drags the model prediction and avoids the main component of the image.</li>
            <li><strong>An intrinsic similarity between the two classes:</strong> For example, mistaking Permanent Crop Land with Annual Crop Land.</li>
        </ol>
        <br><br>
        <img src="images/lr_fsl/eurosat_base_failures.png" alt="Experiments visualization" style="width:100%;">
        <br><br>
        Given this context, the primary issue appears to be related to attention. Indeed, <b>adding LoRA</b> to the base model significantly improves performance. Semantically, the attention maps are now much richer:
        <br><br>
        <img src="images/lr_fsl/zero_vs_lora_eurosat.png" alt="improvement zero_vs_lora_eurosat" style="width:100%;">
        <br><br>
        From a 2D projection perspective, not only are the clusters more separable, but they are also semantically richer. For example, it's noticeable that clusters driven by semantics, such as sea or lake being close to rivers, and different types of land being grouped together, emerge clearly.
        <br><br>
        <img src="images/lr_fsl/umap_plot_lora_eurosat.png" alt="UMAP projection lora on EuroSAT" style="width:100%;">
        <br><br>
        In addition to this, extending the base CLIP model with <b>BitFit</b> also leads to a significant performance improvement. Compared to LoRA, the classification accuracy improvement is especially noticeable for classes such as "Sea or Lake" and "River." We hypothesize that BitFit provides a more comprehensive global understanding of the image, whereas LoRA excels at recognizing fine-grained details. This distinction makes sense given that, in our implementation, LoRA is applied only to the Residual attention blocks of both CLIP's encoders, while BitFit is spread across the entire model, tuning the bias term for each layer.
        <br><br>
        <img src="images/lr_fsl/eurosat_class_acc_table_lora_bitfit.png" alt="acc table lora bitfit eurosat" style="margin-left:12.5%; width:75%;">
        <br><br>
        We also experimented with combining both methods, hoping to achieve a model that performs well on both large-scale details and fine-grained features. However, the result was more of an average of the two characteristics, with the model attending to fine details, but not as much as when LoRA was applied alone.
        <br><br>
        Testing the <b>Meta-Adapter</b> in combination with both LoRA and BitFit produced our best-performing model on EuroSAT, achieving <b>90.95%</b> accuracy. On the other hand, testing the Meta-Adapter alone resulted in underwhelming performance (around 68% accuracy), which aligns with findings in the associated paper, although it still outperforms the baseline. Its main drawback is the intrinsic need to select a support set of images, which can lead to time-wasting or suboptimal choices.
      </p>
      <div class="empty-space"></div>

      <!-- Analysis: Circuit Diagrams  -->
      <h4 style="margin-bottom: 16px;">Analysis: Circuit Diagrams</h4>
      <p>
        The Circuit Diagrams dataset, recently introduced in <b><a href="https://arxiv.org/abs/2401.04716" class="a-light">this paper</a></b>, is a collection of labeled images used for classifying various types of circuit diagrams. These diagrams represent different electrical components and their interconnections, making them highly structured yet visually diverse. The dataset presents unique challenges, such as variations in diagram layout, symbol orientation, and component grouping. To effectively understand the intricacies of this dataset, models must accurately recognize both individual components and the relationships between them.
        <br><br>
        This dataset proved particularly challenging for CLIP, as the base model likely has little to no prior knowledge of such structures, making it an interesting case to study its behavior.
        <br><br>
        Starting with the baseline model, predictions were quite messy, with an overall accuracy of only 12.4%. The main component consistently detected was the <b>text information present in the images</b>, which is a known problem for CLIP.
        <br><br>
        <img src="images/lr_fsl/circuits_failures.png" alt="circuits failures" style="width:100%;">
        <br><br>
        The Circuit Diagrams test set is highly imbalanced, with most classes having only a few samples (around 5 to 10), while a few classes dominate the majority of the test set. <b>Applying LoRA</b> exhibited an unusual behavior: the model collapsed to predict the majority class, still achieving only 18% accuracy. This was unexpected, as the model was trained in a few-shot setting, which should have prevented such behavior by ensuring a fixed number of samples per class. It is possible that the majority class, "converter/power supply/charger/inverter," contains visual features that are particularly easy for the model to identify, though we couldn't fully explain this given our limited background in electronics.
        <br><br>
        On the other hand, the <b>application of BitFit enhanced CLIP’s tendency to focus on textual information in the images</b> without causing the collapse behavior seen with LoRA. This was particularly interesting, as it demonstrated BitFit’s ability to leverage the original model's knowledge, rather than optimizing parameters for a specific task. The bias towards text in images became much stronger with this approach.
        <br><br>
        Our best-performing model configuration (achieving 17.25% accuracy) was obtained through a combination of methods, specifically BitFit, Meta-Adapter, and label preserving/breaking augmentations.
        <br><br>
        <img src="images/lr_fsl/circuits_best_example.png" alt="circuits best example" style="width:100%;">
        <br><br>
        The attention maps clearly show a tendency for the model to avoid the circuit structure and focus instead on blank areas of the images.
      </p>
      <div class="empty-space"></div>

      <!-- Analysis: Meta-Adapter and CLIP Modality Gap  -->
      <h4 style="margin-bottom: 16px;">Analysis: Meta-Adapter and CLIP Modality Gap</h4>
      <p>
        When applying Meta-Adapter either alone or in combination with other methods, we noticed that the model's similarity scores between modalities were much higher than usual. This led us to ask: Does this Meta-Learning strategy also reduce the gap between the text and vision modalities?
        <br><br>
        <img src="images/lr_fsl/modality_gap_metapng.png" alt="modality gap study" style="width:100%;">
        <br><br>
        The answer to this question is "possibly." While the similarity scores are significantly higher, the standard deviation also increases by a few points. This suggests that further investigation is needed to better understand this behavior, ideally using additional datasets.
      </p>
      <div class="empty-space"></div>
      
      <!-- Results  -->
      <h4 style="margin-bottom: 16px;">Results</h4>
      <p>
        The results of applying various methods to the EuroSAT and Circuit Diagrams datasets revealed key insights into model behavior and performance. 
        <br><br>
        For EuroSAT, the baseline CLIP model initially struggled with low accuracy due to the challenge of handling satellite imagery with complex land classifications. However, after incorporating LoRA, a significant improvement was observed, both in terms of cluster separability and the richness of the attention maps. The addition of BitFit further enhanced performance, especially in classes like "Sea or Lake" and "River," where a global understanding of the image was crucial.
        <br><br>
        Combining LoRA, BitFit, and Meta-Adapter yielded the best results, with the model achieving an impressive 90.95% accuracy. This configuration outperformed the individual methods, highlighting the value of a multi-strategy approach.
        <br><br>
        For the Circuit Diagrams dataset, the CLIP model initially faced difficulties due to the highly structured yet visually diverse nature of circuit diagrams, especially with text-heavy images. LoRA caused the model to collapse on the majority class, while BitFit showed more balanced performance by leveraging the model's existing knowledge. The best configuration for Circuit Diagrams was obtained by combining BitFit, Meta-Adapter, and label preserving/breaking augmentations, reaching 17.25% accuracy.
        <br><br>
        The attention maps from the best model configurations in both datasets displayed clear tendencies towards focusing on certain image features. In EuroSAT, the model correctly identified land types and geographical features, while in Circuit Diagrams, the model often ignored the circuit structure, focusing instead on blank spaces.
      </p>
      <div class="empty-space"></div>

      <!-- GitHub  -->
      <div class="empty-space-big"></div>
      <h3 style="text-align: center">Further informations & source code available on GitHub !</h3>
      <div class="empty-space"></div>
      <a href="https://github.com/OmarFacchini/LowResourcesFewShot-CLIP" class="a-light"><img class="image-general" src="images/icons/github.svg" width="64" height="64" alt=""></a>
    </div>
  </div>

  <!-- MY ROLE -->
  <div class="empty-space-big"></div> 
  <div class="section-03">
    <div class="card-orizontal-profile">
      <div style="flex-basis: 65%;" class="item">
        <div>
          <h3 class="titolo-didascalia">My contribution to the project</h3>
          <p class="didascalia">
            My work involved hands-on application of all the proposed methods, along with a detailed analysis of model performance and failure cases across different datasets. I also manually modified the models to collect comprehensive output information, an integral task that required a solid understanding of the entire system. Additionally, the intuition regarding the potential reduction of the modality gap through Meta-Adapter was my own, and I led the investigation into its impact on the model's behavior.
          </p>
          <div class="empty-space"></div>
          <div>
            <a href="projects.html" class="a-light"><button class="button-01"
                style="display: inline-block; margin-bottom: 8px;"> See other projects
              </button> </a>
          </div>
        </div>
      </div>

      <div style="flex-basis: 5%;"></div>

      <div style="flex-basis: 30%; " class="item">
        <img class="image-general" src="images/me/avatar-thinking.png" alt="">
      </div>   
    </div>
  </div>

  <div class="empty-space-big"></div>



  <div class="footer">
    <h2 class="white-text"> Contact me!</h2>
    <h4 class="white-text-secondary">University mail</h4>
    <p class="didascalia, white-text">luca.cazzola-1@studenti.unitn.it</p>
    <h4 class="white-text-secondary">Personal mail</h4>
    <p class="didascalia, white-text">luca.cazzola.2001@gmail.com</p>
    <h4 class="white-text-secondary">Mobile</h4>
    <p class="didascalia, white-text">+39 350 032 3641</p>
    <h4 class="white-text-secondary">Linkedin</h4>
    <p class="didascalia, white-text">luca-cazzola-5699a92a9</p>
  </div>
</body>

</html>