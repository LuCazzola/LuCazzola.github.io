<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&display=swap" rel="stylesheet">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Patua+One&display=swap" rel="stylesheet">

  <title>Test Time Adaptation</title>
  <link rel="icon" type="image/png" sizes="32x32" href="images/favicons/favicon-32x32.png">
  <link rel="stylesheet" href="style.css">
</head>

<body>

  <!-- NAVBAR DESKTOP -->

  <div class="navbar-desktop">
    <a class="a-light" style="margin-right: 5%;" href="index.html">HOME</a>

    <a class="a-light" style="margin-right: 5%;" href="projects.html">PROJECTS</a>

    <a class="a-light" style="margin-right: 5%;" href="bio.html">ABOUT ME</a>

    <a class="a-light" style="margin-right: 5%;" href="contacts.html">CONTACT</a>

  </div>

  <!-- NAVBAR MOBILE -->

  <div class="navbar-mobile">
    <a class="a-light" style="margin-bottom: 8px;" href="index.html">HOME</a>

    <a class="a-light" style="margin-bottom: 8px;" href="projects.html">PROJECTS</a>

    <a class="a-light" style="margin-bottom: 8px;" href="bio.html">ABOUT ME</a>

    <a class="a-light" style="margin-bottom: 8px;" href="contacts.html">CONTACT</a>

  </div>


  <div class="empty-space"></div>

  <div class="section-03">
    <div style="max-width: 600px;">
      <h2 style="margin-bottom: 16px;">TTA : Test Time Adaptation</h2>
      <h3 style="margin-bottom: 16px;">implementation of a TTA solution to improve image classification</h3>
    </div>

    <div class="empty-space"></div>

    <div class="project-overview">
      <h4>Project overview</h4>
      <div style="height: 16px; border-bottom: 1px solid rgba(0, 0, 0, 0.1);
          "> </div>
      <div style="height:16px;"></div>
      <div class="project-overview-content">
        <div class="project-topic">
          <h3>Problems</h3>
          <p>
            Fine tuning a model is not always the solution to improve performances on a specific task.
            <br><br>
            A novel branch of deep learning also known as <b>Test Time Adaptation</b> explores the possibility of improving at test time instead of during training.
          </p>
        </div>
        <div class="project-topic">
          <h3>Methods</h3>
          <p>
            Deep Learning<br><br>
            Data Augmentation<br><br>
            CLIP;<br><br>
            TPT;<br><br>
            CoOp;<br><br>
          </p>  
        </div>
        <div class="project-topic">
          <h3>Tools</h3>
          <p>
            Python;<br><br>
            PyTorch;<br><br>
          </p>
        </div>
        <div class="project-topic">
          <h3>Goals</h3>
          <p>
            Implement a functioning Test Time Adaptation solution improving image classification.
          </p>
        </div>
      </div>
      <div style="height: 16px; border-bottom: 1px solid rgba(0, 0, 0, 0.1);"> </div>
      <div style="height:16px;"></div>
      <h5> Developed in collaboration with Alessandro Lorenzi (2024).</h5>
    </div>

    <div class="empty-space"></div>

    <div style="max-width: 750px;">
      <div class="empty-space"></div>
      <h4 style="margin-bottom: 16px;">Context</h4>
      <p>
        This project was part of the <b>Deep Learning course</b> at University of Trento a.y. 2023/2024, on my first year of Master's degree in AI systems.
        <br><br>
        This represented one of my first times working hands on deep learning architectures, extending its capacities.
      </p>
    </div>
  </div>

  <div class="empty-space-big"></div>

  <!-- PROCESS -->

  <div style="background-color: #dbf1ff">
    <div class="empty-space-big"></div>
    <div class="section-03">
      <h3>Design process</h3>

      <div class="empty-space"></div>

      <div class="project-overview-content">
        <div class="circle">
          <div class="circle-number"></div>
          <h3>1</h3>
          <h4>Implement an existing TTA solution</h4>
        </div>
        <div class="circle">
          <div class="circle-number"></div>
          <h3>2</h3>
          <h4>Try to further improve the model</h4>
        </div>
        <div class="circle">
          <div class="circle-number"></div>
          <h3>3</h3>
          <h4>Draw appropriate conclusions</h4>
        </div>
      </div>
    </div>

    <div class="empty-space-big"></div>

  </div>

  <div class="empty-space-big"></div>

  <div class="section-03">
    <div style="max-width: 750px;">
      <h4 style="margin-bottom: 16px;">The problem</h4>
      <p>
        Test Time Adaptation (TTA) explores the possibility to improve a model's performaces working at test time instead of fine tuning it in a "traditional" way.
        That can be a really effective and helpfull practice mostly for 2 reasons:
        <br><br>
        &#128165; Fine tuning itself might be not so straight forward.
        <br> It really depends on the architecture, but it can be challenging.
        <br><br>
        &#128184; Big models require non neglectable computational capacity & data to work with.
        <br>
        <b>(Lots of money)</b>.
        <br><br>
        Our obective is to implement a possible TTA application improving the performances of an image classifier.
      </p>

      <div class="empty-space"></div>

      <h4 style="margin-bottom: 16px;"> Addressing the problem</h4>
      <p>
        As backbone we opted for <a href="https://openai.com/index/clip/"><b>Contrastive Language–Image Pre-training (CLIP)</b></a>, a well known model by <b>OpenAI</b> trained with the contrastive learning paradigma, capable of making zero-shot
        classification. CLIP works by learning an association between images and relative captions.
        <br><br>
        <img class="image-general" src="images/DeepLearning/CLIP-openAI.png" alt="">
        <br><br>
        The image & text embedding after training is structured such that an image is going to be close to its relative caption. This opens up lots of possibilities!
        If we had a classification task for instance, what we could do (as suggested by OpenAI itself) is :
        <ul>
          <li>Select a set of candidate classes</li>
          <li>For each class define by hand a caption, which might be structured as "a photo of a {class}"</li>
          <li>Plug a query image togheter with all the captions and see if CLIP is able to associate the right caption among all the candidate ones !!!</li>
        </ul>
        This concept of running inference on a task which the model wasn't directly trained for is what we refear as <b>zero-shot capability</b> 
      </p>
    
      <div class="empty-space"></div> 
      
      <h4 style="margin-bottom: 16px;"> TTA baseline</h4>
      <p>
        A possible TTA solution using CLIP as a backbone is <a href="https://arxiv.org/abs/2209.07511"><b>Test-Time Prompt Tuning (TPT)</b></a>.
        <div class="empty-space"></div>
        <img class="image-general" src="images/DeepLearning/TPT.png" alt="">
        <div class="empty-space"></div>
        What TPT does is basically:
        <ul>
          <li>Consider one image at time and augment it N times.</li>
          <li>Push the augmented images and the original one through the CLIP image encoder togheter with a set of prompts.</li>
          <li>Compute the entropy of all augmentations + the original and keep the best 10% (minimizing the entropy).</li>
          <li>Average the top 10% distributions obtaining a marginal distribution, so compute again the (marginal) entropy.</li>
        </ul>
        Prompts can be either handcrafted ("a photo of a {label}" or whatever) or learned via promp learner such as <a href="https://arxiv.org/abs/2109.01134"><b>CoOp</b></a>. Adding a prompt learner also adds the possibility
        to actually use the computed marginal entropy as our model's loss function.
        
        <br><br>

        <table>
          <tr>
            <th>Method</th>
            <th>Avg Accuracy (%)</th>
            <th>Avg Loss (entropy)</th>
          </tr>
          <tr>
            <td>CLIP-RN50 (zero-shot)</td>
            <td>21.88</td>
            <td>2.329</td>
          </tr>
          <tr>
            <td>TPT (handcrafted prompts)</td>
            <td>28.80</td>
            <td>1.919</td>
          </tr>
          <tr>
            <td>TPT + CoOp</td>
            <td><strong><u>29.41</u></strong></td>
            <td><strong><u>1.899</u></strong></td>
          </tr>
        </table>
        <i>(Tests performed on <b>ImageNet-A</b>)</i>
      </p>
      
      <h4 style="margin-bottom: 16px;"> Exploring image / prompt augmentation alternatives </h4>
      <p>
        In order to improve the baseline we've also explored different image augmentation techniques ⭐ :
        <ol>
          <li><b>PreAugment</b> : applies only random crop to the image</li>
          <li><b>AugMix</b> : the method used in the original TPT implementation, technique which mixes randomly generated augmentations and uses a Jensen-Shannon loss to enforce consistency</li>
          <li><b>AutoAugment</b> : a reinforcement learning based method which augment an image according to the one maximizing accuracy (trained on ImageNet)</li>
          <li><b>DiffusionAugment</b> : uses a diffusion model to generate augmentations</li>
        </ol>
      
        <br><br>

        <table>
          <tr>
            <th>Augmentation Technique</th>
            <th>Avg Accuracy (%)</th>
            <th>Avg Loss (entropy)</th>
          </tr>
          <tr>
            <td>PreAugment</td>
            <td>27.51</td>
            <td>3.02041</td>
          </tr>
          <tr>
            <td>AugMix</td>
            <td>28.80</td>
            <td>1.919</td>
          </tr>
          <tr>
            <td>AutoAugment</td>
            <td><strong><u>30.36</u></strong></td>
            <td><strong><u>1.894</u></strong></td>
          </tr>
          <tr>
            <td>DiffusionAugment</td>
            <td><i>**read ahead**</i></td>
            <td><i>**read ahead**</i></td>
          </tr>
        </table>
      
        <br>

        Using AutoAugment we've been able to improve the TPT + CoOp implementation by around 1%, witohut requiring prompt tuning.
        In the case of <b>DiffusionAugment</b> while testing we've realized <u>it is too much expensive (time wise) to generate images online during evaluation for our hardware</u>.
        It takes around 12 sec. for the diffusion model we've selected to perform 25 diffusion steps. Moreover, a single augmentation isn't enough to us and even
        downsampling the number of augmentations to generate from 64 to 10 would still be expensive (2 min. per image times 7500 for ImageNet-A = 250 hours of runtime).
        A work which tests the effectiveness of diffusion models combined with TPT is <a href="https://arxiv.org/abs/2308.06038"><b>DiffTPT</b></a>, in which they avoid the issue of "online generation"
        by basically generating offline augmentations and store them apart ready to be used during inference.
        <i>We consider such solution not really aligned with the goal of TTA as it breaks down the whole principle of improving during inference only</i>.
        For this reason we stopped experimenting with this solution and didn't report any results (other than the code) related to it.

        <br><br>

        We've also been testing a prompt augmentation approach proposed by ourselves which aims to create more context-aware prompts compared to the standard,
        generic descriptions like "a photo of a [class label]." Our hypothesis is that captions specifically tailored to the content of the image will enhance
        the alignment between the image and the class labels, leading to improved model performance.
        
        <br><br>
        <img class="image-general" src="images/DeepLearning/image_captioning_schema.png" alt="prompt_aug_schema">
        <br><br>

        <ol>
            <li><b>Image Captioning</b> :
              We use the VisionEncoderDecoderModel (ViT-GPT2) to generate descriptive captions from the images.
              This model integrates a Vision Transformer (ViT) with GPT-2, allowing it to produce detailed captions that capture the visual content of the images.
            </li>
            <li><b>KeyWords Extraction</b> :
              After generating the caption, we utilize KeyBERT to extract the most relevant keywords or phrases from the caption.
              These keywords represent the key elements or subjects described in the caption.
            </li>
            <li><b>Personalized Prompts composition</b> :
              We replace the most relevant keyword in the caption with each class label from the dataset to create personalized prompts.
              This process generates a set of prompts specific to the content of the image and the class labels.
            </li>
        </ol>

        <br><br>

        <table>
          <tr>
            <th>Method</th>
            <th>Avg Accuracy (%) : CLIP-RN50</th>
            <th>Avg Accuracy (%) : CLIP-ViT-B/16</th>
          </tr>
          <tr>
            <td>Baseline</td>
            <td>21.83</td>
            <td>47.87</td>
          </tr>
          <tr>
            <td>Our Method</td>
            <td>19.41</td>
            <td>42.13</td>
          </tr>
        </table>

        <br><br>

        Despite our hypothesis that contextually specific prompts would improve model performance, the <b>results suggest otherwise</b>.
        The personalized <i>prompts generated by the image captioning system did not achieve better results than the standard approaches</i>.

        <br><br>

        Our prompt augmentation solution basically <u>delegates the handcrafted prompt design to an image captioner model</u>. Such design can potentially be harmful since :
        <ul>
          <li>Performances are dependent on an secondary supervising model (ViT-GPT2 chain in this case) which is detatched from the rest.</li>
          <li>If the input image is noisy, the produced caption will also probably be noisy, making the inference even harder than using a more generic prompt like "a photo of a { label }"<br>As we're evaluating performances on an noisy dataset such as ImageNet-A this might be the most relevant aspect</li>
          <li>As stated by OpenAI itself, CLIP is very sensitive to wording</li>
        </ul>
      </p>

      <div class="empty-space-big"></div>

      <!-- GitHub  -->
      <h3 style="text-align: center">Further informations & source code available on GitHub !</h3>
      <div class="empty-space"></div>
      <a href="https://github.com/LuCazzola/TTA_DL-Project" class="a-light"><img class="image-general" src="images/icons/github.svg" width="64" height="64" alt=""></a>
      
    </div>
  </div>
  
  <div class="empty-space-big"></div> 

  <!-- MY ROLE -->

  <div class="section-03">
    <div class="card-orizontal-profile">
      <div style="flex-basis: 65%;" class="item">
        <div>
          <h3 class="titolo-didascalia">My contribution to the project</h3>
          <p class="didascalia">
            The team worked as a compact unit through almost all the phases of the project.
          </p>
          <div class="empty-space"></div>
          <div>
            <a href="projects.html" class="a-light"><button class="button-01"
                style="display: inline-block; margin-bottom: 8px;"> See other projects
              </button> </a>
          </div>
        </div>
      </div>

      <div style="flex-basis: 5%;"></div>

      <div style="flex-basis: 30%; " class="item">
        <img class="image-general" src="images/me/avatar-thinking.png" alt="">
      </div>   
    </div>
  </div>

  <div class="empty-space-big"></div>

  

  <div class="footer">
    <h2 class="white-text"> Contact me!</h2>
    <h4 class="white-text-secondary">University mail</h4>
    <p class="didascalia, white-text">luca.cazzola-1@studenti.unitn.it</p>
    <h4 class="white-text-secondary">Personal mail</h4>
    <p class="didascalia, white-text">luca.cazzola.2001@gmail.com</p>
    <h4 class="white-text-secondary">Mobile</h4>
    <p class="didascalia, white-text">+39 350 032 3641</p>
    <h4 class="white-text-secondary">Linkedin</h4>
    <p class="didascalia, white-text">luca-cazzola-5699a92a9</p>
  </div>
</body>

</html>